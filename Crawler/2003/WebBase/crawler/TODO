
TODO list for crawler
---------------------
* active item
x no longer applies; "done" because siteserver/dnsdbserver replaced
- done

x siteserver is peppered with exit(1) statements (effectively, abort) 
  in response to network failures.  If dnsdbserver or a crawl_server is 
  not listen(2)ing when siteserver tries to talk to it, siteserver will die.

x dnsdbserver may have a memory leak, causing it to segfault after some time.

* crawl_server does not suggest new sites to siteserver to crawl;
  building a new sitenames (domain-names-to-crawl) list requires scanning
  the old repository.

x siteserver does not save its state; if siteserver restarts, it goes through
  the sitenames list anew.  siteserver also does not edit (append to) 
  the sitenames list.

* siteserver does not implement a deny list (domain names never to crawl).
  To omit domain names, sitenames must be edited.

* crawl_server does not implement (the rarely-implemented) META-tag
  NOINDEX,NOFOLLOW robots exclusion in HTML files it sees.
  (crawl_server does implement robots.txt robot exclusion, as 
  user-agent "Pita".  This is slightly different from how it identifies
  itself currently, "Pita (email-address)" where an email-address is 
  hard-coded at compile-time.)

- crawl_server does not recognize comments, and continues to parse for links
  inside them.

* crawl_server does not check whether URLs it sees are valid URIs (RFC 2396).
  In particular, it may allow \r or \n into URLs written into the repository,
  which may freak out programs expecting URLs to be one line.

x crawl_config cannot change substantially after siteserver is loaded.
  siteserver cannot reread crawl_config on demand.  siteserver's
  data structures are fixed (in size) when siteserver starts up.

* crawl_server has a compiled MAX_NUM_QUEUES that limits the number of Web
  sites crawl_server can crawl concurrently.  This constant cannot be
  exceeded, even if any configuration files request otherwise.

- crawl_server PAUSE may be "leaky:"  If a CRAWL (new site) command arrives
  after a PAUSE, crawl_server will crawl anyway.  A subsequent PAUSE command
  will pause any new sites without losing any old ones, but in the meanwhile, 
  crawl_server can be induced into accumulating more sites to crawl
  concurrently than crawl_server/manager.h:MAX_NUM_QUEUES or
  crawl_config:[starter]:Threads allow.  In this case, Threads would be ignored
  but crawl_server RESUME will drop any sites over MAX_NUM_QUEUES.

* crawl_server can have several pages queued for a Web site, simultaneously.
  The first is /robots.txt, and the second is /, but new pages may then be
  added to the queue before robots.txt is returned and processed.

* crawl_server tries only once to send its Web-site-completed message to its
  siteserver (or crawl_buddy.pl).  If the send fails, e.g. because the Linux
  TCP/IP stack drops packets over localhost (!), the message is permanently
  lost.

- crawl_buddy has a race condition in which it can detect that its crawl_server
  ran out of sites to crawl before it receives the crawl_server's done
  messages for its last Web sites to finish.  To reduce the impact of the
  race condition, make sure crawl_buddy waits for messages a little while
  after it thinks the crawl_server is all done and ready to quit, in case
  the last done messages are in queue.

- crawl_buddy should try LOCK TABLE uncrawled IN ROW EXCLUSIVE MODE to
  avoid fights over the same Web site.  The command is not SQL92, but appears
  in PostgreSQL and Oracle.  It may help...

* crawl_server will crawl a URL if its query parameter is different, which
  may indicate different pages on some sites (...view.php?articleID=1234)
  or the same page over and over again (...doc.html?PHPSESSIONID=0123456).
  Is it possible to take care of the latter case?  One approach is to
  support cookies; then amazon.com will be the only known site affected
  (since amazon.com hides its cookie in the URL path).

* crawl_server seems to busywait (or otherwise consume CPU) when it is paused.

* The crawler has only global, not per-site, settings for the extensions
  (data types) to parse or fetch.

* utils/url.{h,cc} seems to assume at most one parameter per URI, at the end
  of the path.  The grammar in RFC 2396 suggests that there can actually be 
  a parameter for each path segment, including but not restricted to the last.

- crawl_server only checks robots.txt as Pita, not as WebVac.

